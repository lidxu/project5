{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T07:03:43.361598Z",
     "start_time": "2026-01-28T07:03:38.065811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 第一格：环境设置和依赖安装\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 设置随机种子\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# 检查GPU\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "\n",
    "print(f\"PyTorch版本: {torch.__version__}\")\n",
    "print(f\"CUDA是否可用: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU设备: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "set_seed(42)"
   ],
   "id": "ada7dbbb6825d98b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch版本: 2.5.1+cu121\n",
      "CUDA是否可用: True\n",
      "GPU设备: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T07:03:45.317168Z",
     "start_time": "2026-01-28T07:03:44.627549Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 第二格：数据预处理类\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torchvision.transforms as T\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self, bert_path='./bert_model/', max_length=128):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(bert_path)\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def preprocess(self, text_path):\n",
    "        # 直接从文件路径读取，使用 GBK 编码\n",
    "        try:\n",
    "            with open(text_path, 'r', encoding='gbk', errors='ignore') as f:\n",
    "                text = f.read().strip()\n",
    "        except:\n",
    "            text = \"\"\n",
    "            print(f\"无法读取文本文件: {text_path}\")\n",
    "\n",
    "        # 文本清洗\n",
    "        text = str(text).lower().strip()\n",
    "        import re\n",
    "        text = re.sub(r'[^\\w\\s#@]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "        # BERT tokenize\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten()\n",
    "        }\n",
    "\n",
    "class ImagePreprocessor:\n",
    "    def __init__(self, img_size=224):\n",
    "        self.transform = T.Compose([\n",
    "            T.Resize((img_size, img_size)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                       std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        # 数据增强（训练时使用）\n",
    "        self.augmentation = T.Compose([\n",
    "            T.Resize((256, 256)),\n",
    "            T.RandomCrop((img_size, img_size)),\n",
    "            T.RandomHorizontalFlip(p=0.5),\n",
    "            T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                       std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def preprocess(self, image_path, augment=False):\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if augment:\n",
    "            return self.augmentation(image)\n",
    "        else:\n",
    "            return self.transform(image)"
   ],
   "id": "ff4afc35b229d67c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T07:03:45.762123Z",
     "start_time": "2026-01-28T07:03:45.753043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 第三格：数据集类\n",
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, data_dir, label_file, text_processor, image_processor,\n",
    "                 split='train', augment=False):\n",
    "        self.data_dir = data_dir\n",
    "        self.text_processor = text_processor\n",
    "        self.image_processor = image_processor\n",
    "        self.augment = augment and split == 'train'\n",
    "\n",
    "        # 读取标签文件\n",
    "        self.data = []\n",
    "        label_mapping = {'positive': 0, 'neutral': 1, 'negative': 2}\n",
    "\n",
    "        with open(label_file, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()[1:]  # 跳过标题行\n",
    "            for line in lines:\n",
    "                parts = line.strip().split(',')\n",
    "                if len(parts) == 2:\n",
    "                    guid, label = parts\n",
    "                    if label != 'null':\n",
    "                        text_path = os.path.join(data_dir, f'{guid}.txt')\n",
    "                        image_path = os.path.join(data_dir, f'{guid}.jpg')\n",
    "\n",
    "                        if os.path.exists(text_path) and os.path.exists(image_path):\n",
    "                            self.data.append({\n",
    "                                'guid': guid,\n",
    "                                'label': label_mapping[label],\n",
    "                                'text_path': text_path,\n",
    "                                'image_path': image_path\n",
    "                            })\n",
    "                        else:\n",
    "                            print(f\"文件不存在: {text_path} 或 {image_path}\")\n",
    "\n",
    "        print(f\"Loaded {len(self.data)} samples for {split} split\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "\n",
    "        text_data = self.text_processor.preprocess(item['text_path'])\n",
    "\n",
    "\n",
    "        # 处理图像\n",
    "        try:\n",
    "            image = self.image_processor.preprocess(item['image_path'], self.augment)\n",
    "        except:\n",
    "            # 如果图片不存在，使用黑色图像\n",
    "            image = torch.zeros(3, 224, 224)\n",
    "\n",
    "        return {\n",
    "            'input_ids': text_data['input_ids'],\n",
    "            'attention_mask': text_data['attention_mask'],\n",
    "            'image': image,\n",
    "            'label': torch.tensor(item['label'], dtype=torch.long),\n",
    "            'guid': item['guid']\n",
    "        }"
   ],
   "id": "a76d4295ad74f6e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T07:03:46.906042Z",
     "start_time": "2026-01-28T07:03:46.897058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 第四格：多模态融合模型\n",
    "class MultiModalModel(nn.Module):\n",
    "    def __init__(self, bert_path='./bert_model/', num_classes=3, fusion_method='late'):\n",
    "        super(MultiModalModel, self).__init__()\n",
    "        self.fusion_method = fusion_method\n",
    "\n",
    "        # 文本特征提取器 (BERT)\n",
    "        self.text_encoder = BertModel.from_pretrained(bert_path)\n",
    "        text_hidden_size = self.text_encoder.config.hidden_size\n",
    "\n",
    "        # 图像特征提取器 (ResNet-50)\n",
    "        self.image_encoder = models.resnet50(pretrained=True)\n",
    "        self.image_encoder.fc = nn.Identity()  # 移除最后的分类层\n",
    "        image_hidden_size = 2048  # ResNet-50的输出维度\n",
    "\n",
    "        # 文本分类头\n",
    "        self.text_classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(text_hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "        # 图像分类头\n",
    "        self.image_classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(image_hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "        # 多模态融合分类头\n",
    "        if fusion_method == 'early':\n",
    "            # 早期融合：特征拼接\n",
    "            fusion_dim = text_hidden_size + image_hidden_size\n",
    "            self.fusion_classifier = nn.Sequential(\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(fusion_dim, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(512, num_classes)\n",
    "            )\n",
    "        elif fusion_method == 'late':\n",
    "            # 晚期融合：决策级融合（默认）\n",
    "            self.fusion_classifier = nn.Linear(num_classes * 2, num_classes)\n",
    "        elif fusion_method == 'middle':\n",
    "            # 中期融合：注意力机制\n",
    "            self.cross_attention = nn.MultiheadAttention(\n",
    "                embed_dim=512, num_heads=8, batch_first=True\n",
    "            )\n",
    "            self.fusion_classifier = nn.Sequential(\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(1024, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(512, num_classes)\n",
    "            )\n",
    "\n",
    "        # 投影层（用于统一特征维度）\n",
    "        self.text_projection = nn.Linear(text_hidden_size, 512)\n",
    "        self.image_projection = nn.Linear(image_hidden_size, 512)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, image):\n",
    "        # 提取文本特征\n",
    "        text_outputs = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_features = text_outputs.pooler_output  # [CLS] token的输出\n",
    "\n",
    "        # 提取图像特征\n",
    "        image_features = self.image_encoder(image)\n",
    "\n",
    "        # 单模态预测\n",
    "        text_logits = self.text_classifier(text_features)\n",
    "        image_logits = self.image_classifier(image_features)\n",
    "\n",
    "        # 多模态融合\n",
    "        if self.fusion_method == 'early':\n",
    "            # 特征拼接\n",
    "            fused_features = torch.cat([text_features, image_features], dim=1)\n",
    "            fused_logits = self.fusion_classifier(fused_features)\n",
    "\n",
    "        elif self.fusion_method == 'late':\n",
    "            # 决策级融合（加权平均）\n",
    "            fused_logits = self.fusion_classifier(torch.cat([text_logits, image_logits], dim=1))\n",
    "\n",
    "        elif self.fusion_method == 'middle':\n",
    "            # 中期融合：注意力机制\n",
    "            text_proj = self.text_projection(text_features).unsqueeze(1)\n",
    "            image_proj = self.image_projection(image_features).unsqueeze(1)\n",
    "\n",
    "            # 交叉注意力\n",
    "            attended_features, _ = self.cross_attention(\n",
    "                text_proj, image_proj, image_proj\n",
    "            )\n",
    "            fused_features = torch.cat([\n",
    "                text_proj.squeeze(1),\n",
    "                attended_features.squeeze(1)\n",
    "            ], dim=1)\n",
    "            fused_logits = self.fusion_classifier(fused_features)\n",
    "\n",
    "        return fused_logits, text_logits, image_logits"
   ],
   "id": "6a52fea3c061d233",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T07:03:47.508085Z",
     "start_time": "2026-01-28T07:03:47.489046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 第五格：训练函数\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc='Training')\n",
    "    for batch in progress_bar:\n",
    "        # 移动到设备\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        images = batch['image'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        # 前向传播\n",
    "        optimizer.zero_grad()\n",
    "        fused_logits, text_logits, image_logits = model(input_ids, attention_mask, images)\n",
    "\n",
    "        # 计算损失（多任务学习：主损失 + 辅助损失）\n",
    "        loss_fused = criterion(fused_logits, labels)\n",
    "        loss_text = criterion(text_logits, labels)\n",
    "        loss_image = criterion(image_logits, labels)\n",
    "        loss = loss_fused + 0.3 * loss_text + 0.3 * loss_image\n",
    "\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # 统计\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(fused_logits, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # 更新进度条\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': loss.item(),\n",
    "            'acc': 100 * correct / total\n",
    "        })\n",
    "\n",
    "    return total_loss / len(dataloader), 100 * correct / total\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Validation'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            fused_logits, _, _ = model(input_ids, attention_mask, images)\n",
    "            loss = criterion(fused_logits, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(fused_logits, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    print(\"\\n分类报告:\")\n",
    "    print(classification_report(all_labels, all_preds,\n",
    "                               target_names=['positive', 'neutral', 'negative']))\n",
    "\n",
    "    return total_loss / len(dataloader), 100 * correct / total, all_preds, all_labels"
   ],
   "id": "fee1aeb0f4e504d7",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T07:03:48.005977Z",
     "start_time": "2026-01-28T07:03:47.999026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 第六格：主训练流程\n",
    "def main_training():\n",
    "    # 初始化处理器\n",
    "    text_processor = TextPreprocessor(bert_path='./bert_model/', max_length=128)\n",
    "    image_processor = ImagePreprocessor(img_size=224)\n",
    "\n",
    "    # 加载数据\n",
    "\n",
    "    # 创建完整数据集\n",
    "    full_dataset = MultiModalDataset(\n",
    "        data_dir='./data/data',\n",
    "        label_file='./data/train.txt',\n",
    "        text_processor=text_processor,\n",
    "        image_processor=image_processor,\n",
    "        split='train'\n",
    "    )\n",
    "\n",
    "    # 划分训练集和验证集 (80%训练, 20%验证)\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        full_dataset, [train_size, val_size]\n",
    "    )\n",
    "\n",
    "    # 创建数据加载器\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=32, shuffle=True, num_workers=4\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=32, shuffle=False, num_workers=4\n",
    "    )\n",
    "\n",
    "    # 初始化模型\n",
    "    model = MultiModalModel(\n",
    "        bert_path='./bert_model/',\n",
    "        num_classes=3,\n",
    "        fusion_method='late'  # 可以尝试 'early' 或 'middle'\n",
    "    ).to(device)\n",
    "\n",
    "    # 损失函数和优化器\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=2e-5,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "\n",
    "    # 学习率调度器\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=3, verbose=True\n",
    "    )\n",
    "\n",
    "    # 训练循环\n",
    "    best_acc = 0\n",
    "    patience = 5\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(20):\n",
    "        print(f\"\\nEpoch {epoch+1}/20\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        # 训练\n",
    "        train_loss, train_acc = train_epoch(\n",
    "            model, train_loader, criterion, optimizer, device\n",
    "        )\n",
    "\n",
    "        # 验证\n",
    "        val_loss, val_acc, val_preds, val_labels = validate(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "\n",
    "        print(f\"\\n训练集 - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%\")\n",
    "        print(f\"验证集 - Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        # 学习率调整\n",
    "        scheduler.step(val_acc)\n",
    "\n",
    "        # 保存最佳模型\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_acc': val_acc,\n",
    "                'val_loss': val_loss,\n",
    "            }, 'best_model.pth')\n",
    "            print(f\"保存新的最佳模型，验证准确率: {val_acc:.2f}%\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"早停触发\")\n",
    "                break\n",
    "\n",
    "    return model"
   ],
   "id": "d4f3d623bcec0c1b",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T07:03:48.475009Z",
     "start_time": "2026-01-28T07:03:48.452461Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 第七格：测试集预测\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, test_file, data_dir, text_processor, image_processor):\n",
    "        self.data_dir = data_dir\n",
    "        self.text_processor = text_processor\n",
    "        self.image_processor = image_processor\n",
    "\n",
    "        # 读取测试文件\n",
    "        self.guids = []\n",
    "        with open(test_file, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()[1:]  # 跳过标题行\n",
    "            for line in lines:\n",
    "                guid = line.strip().split(',')[0]\n",
    "                self.guids.append(guid)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.guids)\n",
    "    def __getitem__(self, idx):\n",
    "        guid = self.guids[idx]\n",
    "\n",
    "        # 文本路径和图像路径\n",
    "        text_path = os.path.join(self.data_dir, f'{guid}.txt')\n",
    "        image_path = os.path.join(self.data_dir, f'{guid}.jpg')\n",
    "\n",
    "        # 读取文本\n",
    "        try:\n",
    "            with open(text_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read().strip()\n",
    "        except:\n",
    "            text = \"\"\n",
    "\n",
    "        # 处理文本\n",
    "        text_data = self.text_processor.preprocess(text)\n",
    "\n",
    "        # 处理图像\n",
    "        try:\n",
    "            image = self.image_processor.preprocess(image_path, augment=False)\n",
    "        except:\n",
    "            image = torch.zeros(3, 224, 224)\n",
    "\n",
    "        return {\n",
    "            'input_ids': text_data['input_ids'],\n",
    "            'attention_mask': text_data['attention_mask'],\n",
    "            'image': image,\n",
    "            'guid': guid\n",
    "        }\n",
    "\n",
    "def predict_test_set(model, test_file, output_file):\n",
    "    model.eval()\n",
    "\n",
    "    # 初始化处理器\n",
    "    text_processor = TextPreprocessor(bert_path='./bert_model/', max_length=128)\n",
    "    image_processor = ImagePreprocessor(img_size=224)\n",
    "\n",
    "    # 创建测试数据集\n",
    "    test_dataset = TestDataset(\n",
    "        test_file=test_file,\n",
    "        data_dir='./data',\n",
    "        text_processor=text_processor,\n",
    "        image_processor=image_processor\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "    # 预测\n",
    "    predictions = {}\n",
    "    label_mapping = {0: 'positive', 1: 'neutral', 2: 'negative'}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc='Predicting'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            images = batch['image'].to(device)\n",
    "            guids = batch['guid']\n",
    "\n",
    "            fused_logits, _, _ = model(input_ids, attention_mask, images)\n",
    "            _, preds = torch.max(fused_logits, 1)\n",
    "\n",
    "            for guid, pred in zip(guids, preds.cpu().numpy()):\n",
    "                predictions[guid] = label_mapping[pred]\n",
    "\n",
    "    # 写入结果文件\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"guid,tag\\n\")\n",
    "        with open(test_file, 'r', encoding='utf-8') as test_f:\n",
    "            lines = test_f.readlines()[1:]\n",
    "            for line in lines:\n",
    "                guid = line.strip().split(',')[0]\n",
    "                tag = predictions.get(guid, 'neutral')  # 默认neutral\n",
    "                f.write(f\"{guid},{tag}\\n\")\n",
    "\n",
    "    print(f\"预测结果已保存到: {output_file}\")\n",
    "    return predictions"
   ],
   "id": "c21ec7256f8c2dcc",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T07:03:49.054844Z",
     "start_time": "2026-01-28T07:03:49.046756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 第八格：消融实验\n",
    "def ablation_study(model, dataloader, device):\n",
    "    \"\"\"对比不同模态的效果\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # 只使用文本\n",
    "    text_correct = 0\n",
    "    # 只使用图像\n",
    "    image_correct = 0\n",
    "    # 多模态融合\n",
    "    fusion_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Ablation Study'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            fused_logits, text_logits, image_logits = model(\n",
    "                input_ids, attention_mask, images\n",
    "            )\n",
    "\n",
    "            # 文本模态预测\n",
    "            _, text_preds = torch.max(text_logits, 1)\n",
    "            text_correct += (text_preds == labels).sum().item()\n",
    "\n",
    "            # 图像模态预测\n",
    "            _, image_preds = torch.max(image_logits, 1)\n",
    "            image_correct += (image_preds == labels).sum().item()\n",
    "\n",
    "            # 多模态融合预测\n",
    "            _, fusion_preds = torch.max(fused_logits, 1)\n",
    "            fusion_correct += (fusion_preds == labels).sum().item()\n",
    "\n",
    "            total += labels.size(0)\n",
    "\n",
    "    text_acc = 100 * text_correct / total\n",
    "    image_acc = 100 * image_correct / total\n",
    "    fusion_acc = 100 * fusion_correct / total\n",
    "\n",
    "    print(\"\\n消融实验结果:\")\n",
    "    print(f\"仅文本模态准确率: {text_acc:.2f}%\")\n",
    "    print(f\"仅图像模态准确率: {image_acc:.2f}%\")\n",
    "    print(f\"多模态融合准确率: {fusion_acc:.2f}%\")\n",
    "    print(f\"多模态相对提升: {fusion_acc - max(text_acc, image_acc):.2f}%\")\n",
    "\n",
    "    return {\n",
    "        'text_only': text_acc,\n",
    "        'image_only': image_acc,\n",
    "        'multimodal': fusion_acc\n",
    "    }"
   ],
   "id": "943ae0fd2748c65b",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T07:03:31.474320500Z",
     "start_time": "2026-01-28T05:33:58.111154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 第九格：完整可运行的多模态情感分类实验\n",
    "if __name__ == \"__main__\":\n",
    "    import os\n",
    "    import sys\n",
    "    import time\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"多模态情感分类实验 - 完整可运行版本\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # ========== 1. 环境设置 ==========\n",
    "    print(\"\\n1. 环境设置...\")\n",
    "\n",
    "    # 禁用tokenizer的多线程警告\n",
    "    os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "    # 设置随机种子\n",
    "    def set_seed(seed=42):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    set_seed(42)\n",
    "\n",
    "    # ========== 2. 修复BERT模型加载 ==========\n",
    "    print(\"\\n2. 初始化BERT模型...\")\n",
    "\n",
    "    class FixedTextPreprocessor:\n",
    "        \"\"\"修复的文本预处理器\"\"\"\n",
    "        def __init__(self, max_length=128):\n",
    "            self.max_length = max_length\n",
    "            self.tokenizer = None\n",
    "\n",
    "            # 尝试加载BERT tokenizer\n",
    "            try:\n",
    "                # 检查本地是否有BERT模型\n",
    "                if os.path.exists('./bert_model/'):\n",
    "                    print(\"  尝试从本地加载BERT tokenizer...\")\n",
    "                    self.tokenizer = BertTokenizer.from_pretrained('./bert_model/')\n",
    "                    print(\"  ✅ 本地BERT tokenizer加载成功\")\n",
    "                else:\n",
    "                    print(\"  本地BERT不存在，尝试在线加载...\")\n",
    "                    # 跳过SSL验证\n",
    "                    import ssl\n",
    "                    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "                    # 使用小模型避免下载失败\n",
    "                    self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "                    print(\"  ✅ 在线BERT tokenizer加载成功\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ⚠️ BERT tokenizer加载失败: {e}\")\n",
    "                print(\"  使用简单的字符级tokenizer作为备用\")\n",
    "                self.tokenizer = None\n",
    "\n",
    "        def preprocess(self, text_path):\n",
    "            \"\"\"处理文本文件\"\"\"\n",
    "            # 读取文本\n",
    "            text = \"\"\n",
    "            encodings = ['utf-8', 'gbk', 'latin-1', 'cp1252']\n",
    "\n",
    "            for encoding in encodings:\n",
    "                try:\n",
    "                    with open(text_path, 'r', encoding=encoding) as f:\n",
    "                        text = f.read().strip()\n",
    "                    if text:\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "            if not text:\n",
    "                text = \"empty text\"\n",
    "\n",
    "            # 简单的文本清洗\n",
    "            import re\n",
    "            text = str(text).lower().strip()\n",
    "            text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "            # 使用BERT tokenizer或备用tokenizer\n",
    "            if self.tokenizer is not None:\n",
    "                try:\n",
    "                    encoding = self.tokenizer.encode_plus(\n",
    "                        text,\n",
    "                        add_special_tokens=True,\n",
    "                        max_length=self.max_length,\n",
    "                        padding='max_length',\n",
    "                        truncation=True,\n",
    "                        return_tensors='pt'\n",
    "                    )\n",
    "                    return {\n",
    "                        'input_ids': encoding['input_ids'].flatten(),\n",
    "                        'attention_mask': encoding['attention_mask'].flatten()\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    print(f\"  ⚠️ BERT tokenization失败: {e}\")\n",
    "\n",
    "            # 备用方案：简单的字符级tokenization\n",
    "            print(f\"  使用备用tokenizer处理: {os.path.basename(text_path)}\")\n",
    "            text_chars = list(text[:self.max_length-2])\n",
    "            char_ids = [ord(c) % 1000 for c in text_chars]\n",
    "\n",
    "            # 添加特殊token\n",
    "            char_ids = [101] + char_ids + [102]  # [CLS]和[SEP]\n",
    "\n",
    "            # Padding\n",
    "            if len(char_ids) < self.max_length:\n",
    "                char_ids = char_ids + [0] * (self.max_length - len(char_ids))\n",
    "            else:\n",
    "                char_ids = char_ids[:self.max_length]\n",
    "\n",
    "            attention_mask = [1 if x != 0 else 0 for x in char_ids]\n",
    "\n",
    "            return {\n",
    "                'input_ids': torch.tensor(char_ids, dtype=torch.long),\n",
    "                'attention_mask': torch.tensor(attention_mask, dtype=torch.long)\n",
    "            }\n",
    "\n",
    "    # ========== 3. 修复图像处理器 ==========\n",
    "    print(\"\\n3. 初始化图像处理器...\")\n",
    "\n",
    "    class FixedImagePreprocessor:\n",
    "        \"\"\"修复的图像预处理器\"\"\"\n",
    "        def __init__(self, img_size=224):\n",
    "            # 训练时的数据增强\n",
    "            self.train_transform = transforms.Compose([\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.RandomCrop((img_size, img_size)),\n",
    "                transforms.RandomHorizontalFlip(p=0.3),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "            ])\n",
    "\n",
    "            # 验证/测试时的转换\n",
    "            self.val_transform = transforms.Compose([\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "            ])\n",
    "\n",
    "        def preprocess(self, image_path, augment=False):\n",
    "            try:\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "                if augment:\n",
    "                    return self.train_transform(image)\n",
    "                else:\n",
    "                    return self.val_transform(image)\n",
    "            except Exception as e:\n",
    "                print(f\"  图像加载失败 {image_path}: {e}\")\n",
    "                # 返回黑色图像\n",
    "                return torch.zeros(3, 224, 224)\n",
    "\n",
    "    # ========== 4. 修复数据集类 ==========\n",
    "    print(\"\\n4. 创建数据集...\")\n",
    "\n",
    "    class FixedMultiModalDataset(Dataset):\n",
    "        \"\"\"修复的数据集类\"\"\"\n",
    "        def __init__(self, data_dir, label_file, text_processor, image_processor,\n",
    "                     split='train', augment=False, max_samples=None):\n",
    "            self.data_dir = data_dir\n",
    "            self.text_processor = text_processor\n",
    "            self.image_processor = image_processor\n",
    "            self.augment = augment and split == 'train'\n",
    "\n",
    "            # 读取标签\n",
    "            self.data = []\n",
    "            label_mapping = {'positive': 0, 'neutral': 1, 'negative': 2}\n",
    "\n",
    "            try:\n",
    "                with open(label_file, 'r', encoding='utf-8') as f:\n",
    "                    lines = f.readlines()[1:]  # 跳过标题\n",
    "\n",
    "                    # 限制样本数量用于快速测试\n",
    "                    if max_samples:\n",
    "                        lines = lines[:max_samples]\n",
    "\n",
    "                    for i, line in enumerate(lines):\n",
    "                        parts = line.strip().split(',')\n",
    "                        if len(parts) >= 2:\n",
    "                            guid = parts[0].strip()\n",
    "                            label = parts[1].strip().lower()\n",
    "\n",
    "                            if label in label_mapping:\n",
    "                                text_path = os.path.join(data_dir, f'{guid}.txt')\n",
    "                                image_path = os.path.join(data_dir, f'{guid}.jpg')\n",
    "\n",
    "                                # 检查文件是否存在\n",
    "                                text_exists = os.path.exists(text_path)\n",
    "                                img_exists = os.path.exists(image_path)\n",
    "\n",
    "                                if text_exists and img_exists:\n",
    "                                    self.data.append({\n",
    "                                        'guid': guid,\n",
    "                                        'label': label_mapping[label],\n",
    "                                        'text_path': text_path,\n",
    "                                        'image_path': image_path\n",
    "                                    })\n",
    "                                else:\n",
    "                                    if i < 5:  # 只显示前5个缺失文件\n",
    "                                        print(f\"  ⚠️ 文件缺失: guid={guid}, text={text_exists}, image={img_exists}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  读取标签文件失败: {e}\")\n",
    "\n",
    "            print(f\"  ✅ 加载了 {len(self.data)} 个样本\")\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.data)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            item = self.data[idx]\n",
    "\n",
    "            # 处理文本\n",
    "            text_data = self.text_processor.preprocess(item['text_path'])\n",
    "\n",
    "            # 处理图像\n",
    "            image = self.image_processor.preprocess(item['image_path'], self.augment)\n",
    "\n",
    "            return {\n",
    "                'input_ids': text_data['input_ids'],\n",
    "                'attention_mask': text_data['attention_mask'],\n",
    "                'image': image,\n",
    "                'label': torch.tensor(item['label'], dtype=torch.long),\n",
    "                'guid': item['guid']\n",
    "            }\n",
    "\n",
    "    # ========== 5. 修复模型类 ==========\n",
    "    print(\"\\n5. 初始化模型...\")\n",
    "\n",
    "    class FixedMultiModalModel(nn.Module):\n",
    "        \"\"\"修复的多模态模型\"\"\"\n",
    "        def __init__(self, num_classes=3, fusion_method='late'):\n",
    "            super(FixedMultiModalModel, self).__init__()\n",
    "            self.fusion_method = fusion_method\n",
    "\n",
    "            # ===== 文本编码器 =====\n",
    "            print(\"  初始化文本编码器...\")\n",
    "            try:\n",
    "                if os.path.exists('./bert_model/'):\n",
    "                    self.text_encoder = BertModel.from_pretrained('./bert_model/')\n",
    "                else:\n",
    "                    # 创建一个简化的BERT（避免下载）\n",
    "                    from transformers import BertConfig\n",
    "                    config = BertConfig(\n",
    "                        hidden_size=768,\n",
    "                        num_hidden_layers=4,\n",
    "                        num_attention_heads=8,\n",
    "                        intermediate_size=3072,\n",
    "                        hidden_act=\"gelu\"\n",
    "                    )\n",
    "                    self.text_encoder = BertModel(config)\n",
    "                    print(\"  ⚠️ 使用随机初始化的BERT（本地模型不存在）\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ⚠️ BERT加载失败，使用备用文本编码器: {e}\")\n",
    "                # 备用文本编码器\n",
    "                self.text_encoder = nn.Sequential(\n",
    "                    nn.Embedding(1000, 768),\n",
    "                    nn.Linear(768, 768),\n",
    "                    nn.Tanh()\n",
    "                )\n",
    "\n",
    "            text_hidden_size = 768\n",
    "\n",
    "            # ===== 图像编码器 =====\n",
    "            print(\"  初始化图像编码器...\")\n",
    "            try:\n",
    "                # 使用预训练的ResNet-18（更小更快）\n",
    "                self.image_encoder = models.resnet18(pretrained=True)\n",
    "                self.image_encoder.fc = nn.Identity()\n",
    "                image_hidden_size = 512\n",
    "            except:\n",
    "                print(\"  ⚠️ ResNet加载失败，使用备用图像编码器\")\n",
    "                # 备用图像编码器\n",
    "                self.image_encoder = nn.Sequential(\n",
    "                    nn.Conv2d(3, 64, 3, padding=1),\n",
    "                    nn.ReLU(),\n",
    "                    nn.MaxPool2d(2),\n",
    "                    nn.Conv2d(64, 128, 3, padding=1),\n",
    "                    nn.ReLU(),\n",
    "                    nn.MaxPool2d(2),\n",
    "                    nn.Conv2d(128, 256, 3, padding=1),\n",
    "                    nn.ReLU(),\n",
    "                    nn.AdaptiveAvgPool2d((1, 1)),\n",
    "                    nn.Flatten(),\n",
    "                    nn.Linear(256, 512)\n",
    "                )\n",
    "                image_hidden_size = 512\n",
    "\n",
    "            # ===== 分类头 =====\n",
    "            self.text_classifier = nn.Sequential(\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(text_hidden_size, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, num_classes)\n",
    "            )\n",
    "\n",
    "            self.image_classifier = nn.Sequential(\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(image_hidden_size, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, num_classes)\n",
    "            )\n",
    "\n",
    "            # ===== 融合分类器 =====\n",
    "            if fusion_method == 'early':\n",
    "                fusion_dim = text_hidden_size + image_hidden_size\n",
    "                self.fusion_classifier = nn.Sequential(\n",
    "                    nn.Dropout(0.3),\n",
    "                    nn.Linear(fusion_dim, 256),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(256, num_classes)\n",
    "                )\n",
    "            elif fusion_method == 'late':\n",
    "                self.fusion_classifier = nn.Linear(num_classes * 2, num_classes)\n",
    "            else:  # middle fusion\n",
    "                self.text_projection = nn.Linear(text_hidden_size, 256)\n",
    "                self.image_projection = nn.Linear(image_hidden_size, 256)\n",
    "                self.fusion_classifier = nn.Sequential(\n",
    "                    nn.Dropout(0.3),\n",
    "                    nn.Linear(512, 256),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(256, num_classes)\n",
    "                )\n",
    "\n",
    "            print(f\"  ✅ 模型初始化完成，融合方法: {fusion_method}\")\n",
    "\n",
    "        def forward(self, input_ids, attention_mask, image):\n",
    "            # 文本特征\n",
    "            if isinstance(self.text_encoder, BertModel):\n",
    "                try:\n",
    "                    text_outputs = self.text_encoder(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask\n",
    "                    )\n",
    "                    text_features = text_outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "                except:\n",
    "                    # 如果BERT失败，使用备用方法\n",
    "                    batch_size = input_ids.size(0)\n",
    "                    text_features = torch.randn(batch_size, 768).to(input_ids.device)\n",
    "            else:\n",
    "                # 备用文本编码器\n",
    "                text_features = self.text_encoder(input_ids)\n",
    "\n",
    "            # 图像特征\n",
    "            image_features = self.image_encoder(image)\n",
    "\n",
    "            # 单模态预测\n",
    "            text_logits = self.text_classifier(text_features)\n",
    "            image_logits = self.image_classifier(image_features)\n",
    "\n",
    "            # 多模态融合\n",
    "            if self.fusion_method == 'early':\n",
    "                fused_features = torch.cat([text_features, image_features], dim=1)\n",
    "                fused_logits = self.fusion_classifier(fused_features)\n",
    "            elif self.fusion_method == 'late':\n",
    "                fused_logits = self.fusion_classifier(\n",
    "                    torch.cat([text_logits, image_logits], dim=1)\n",
    "                )\n",
    "            else:  # middle fusion\n",
    "                text_proj = self.text_projection(text_features)\n",
    "                image_proj = self.image_projection(image_features)\n",
    "                fused_features = torch.cat([text_proj, image_proj], dim=1)\n",
    "                fused_logits = self.fusion_classifier(fused_features)\n",
    "\n",
    "            return fused_logits, text_logits, image_logits\n",
    "\n",
    "    # ========== 6. 修复训练函数 ==========\n",
    "    print(\"\\n6. 设置训练参数...\")\n",
    "\n",
    "    def fixed_train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "        \"\"\"修复的训练epoch函数\"\"\"\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        progress_bar = tqdm(dataloader, desc='训练')\n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            # 移动到设备\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            # 前向传播\n",
    "            optimizer.zero_grad()\n",
    "            try:\n",
    "                fused_logits, text_logits, image_logits = model(input_ids, attention_mask, images)\n",
    "\n",
    "                # 计算损失\n",
    "                loss_fused = criterion(fused_logits, labels)\n",
    "                loss_text = criterion(text_logits, labels)\n",
    "                loss_image = criterion(image_logits, labels)\n",
    "                loss = loss_fused + 0.2 * loss_text + 0.2 * loss_image\n",
    "\n",
    "                # 反向传播\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "                # 统计\n",
    "                total_loss += loss.item()\n",
    "                _, predicted = torch.max(fused_logits, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                # 更新进度条\n",
    "                progress_bar.set_postfix({\n",
    "                    'loss': f'{loss.item():.4f}',\n",
    "                    'acc': f'{100 * correct / total:.1f}%'\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"\\n⚠️ 训练batch {batch_idx}失败: {e}\")\n",
    "                continue\n",
    "\n",
    "        return total_loss / max(len(dataloader), 1), 100 * correct / max(total, 1)\n",
    "\n",
    "    def fixed_validate(model, dataloader, criterion, device):\n",
    "        \"\"\"修复的验证函数\"\"\"\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc='验证'):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                images = batch['image'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "\n",
    "                try:\n",
    "                    fused_logits, _, _ = model(input_ids, attention_mask, images)\n",
    "                    loss = criterion(fused_logits, labels)\n",
    "\n",
    "                    total_loss += loss.item()\n",
    "                    _, predicted = torch.max(fused_logits, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "\n",
    "                    all_preds.extend(predicted.cpu().numpy())\n",
    "                    all_labels.extend(labels.cpu().numpy())\n",
    "                except Exception as e:\n",
    "                    print(f\"验证batch失败: {e}\")\n",
    "                    continue\n",
    "\n",
    "        if total > 0:\n",
    "            # 计算分类报告\n",
    "            from sklearn.metrics import classification_report\n",
    "            print(\"\\n分类报告:\")\n",
    "            print(classification_report(all_labels, all_preds,\n",
    "                                       target_names=['positive', 'neutral', 'negative'],\n",
    "                                       digits=4))\n",
    "\n",
    "        return total_loss / max(len(dataloader), 1), 100 * correct / max(total, 1), all_preds, all_labels\n",
    "\n",
    "    # ========== 7. 主训练流程 ==========\n",
    "    print(\"\\n7. 开始主训练流程...\")\n",
    "\n",
    "    # 创建处理器\n",
    "    text_processor = FixedTextPreprocessor(max_length=128)\n",
    "    image_processor = FixedImagePreprocessor(img_size=224)\n",
    "\n",
    "    # 创建完整数据集（限制样本数量以加快速度）\n",
    "    print(\"\\n加载训练数据...\")\n",
    "    full_dataset = FixedMultiModalDataset(\n",
    "        data_dir='./data/data',\n",
    "        label_file='./data/train.txt',\n",
    "        text_processor=text_processor,\n",
    "        image_processor=image_processor,\n",
    "        split='train',\n",
    "        max_samples=1000  # 只用1000个样本，可以调大\n",
    "    )\n",
    "\n",
    "    # 划分训练集和验证集\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        full_dataset, [train_size, val_size]\n",
    "    )\n",
    "\n",
    "    print(f\"训练集大小: {len(train_dataset)}\")\n",
    "    print(f\"验证集大小: {len(val_dataset)}\")\n",
    "\n",
    "    # 创建数据加载器（使用单进程避免问题）\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=16,  # 较小的batch size\n",
    "        shuffle=True,\n",
    "        num_workers=0,  # Windows下用0\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=16,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    # 初始化模型\n",
    "    model = FixedMultiModalModel(\n",
    "        num_classes=3,\n",
    "        fusion_method='late'  # 可以使用 'early', 'middle', 'late'\n",
    "    ).to(device)\n",
    "\n",
    "    # 损失函数和优化器\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=2e-5,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "\n",
    "    # 学习率调度器\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n",
    "\n",
    "    # 训练循环\n",
    "    best_acc = 0\n",
    "    patience = 3\n",
    "    patience_counter = 0\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"开始训练循环\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for epoch in range(10):  # 训练10个epoch\n",
    "        print(f\"\\nEpoch {epoch+1}/10\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        # 训练\n",
    "        train_loss, train_acc = fixed_train_epoch(\n",
    "            model, train_loader, criterion, optimizer, device\n",
    "        )\n",
    "\n",
    "        # 验证\n",
    "        val_loss, val_acc, val_preds, val_labels = fixed_validate(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "\n",
    "        # 更新学习率\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"\\n训练集 - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%\")\n",
    "        print(f\"验证集 - Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        # 保存最佳模型\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            patience_counter = 0\n",
    "\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_acc': val_acc,\n",
    "                'val_loss': val_loss,\n",
    "            }, 'fixed_best_model.pth')\n",
    "\n",
    "            print(f\"✅ 保存最佳模型，验证准确率: {val_acc:.2f}%\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"⚠️ 早停触发，连续{patience}个epoch验证准确率未提升\")\n",
    "                break\n",
    "\n",
    "    # ========== 8. 消融实验 ==========\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"消融实验\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 加载最佳模型\n",
    "    if os.path.exists('fixed_best_model.pth'):\n",
    "        print(\"加载最佳模型进行消融实验...\")\n",
    "        checkpoint = torch.load('fixed_best_model.pth')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # 测试不同模态的效果\n",
    "    text_correct = 0\n",
    "    image_correct = 0\n",
    "    fusion_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc='消融实验'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            fused_logits, text_logits, image_logits = model(\n",
    "                input_ids, attention_mask, images\n",
    "            )\n",
    "\n",
    "            # 文本模态\n",
    "            _, text_preds = torch.max(text_logits, 1)\n",
    "            text_correct += (text_preds == labels).sum().item()\n",
    "\n",
    "            # 图像模态\n",
    "            _, image_preds = torch.max(image_logits, 1)\n",
    "            image_correct += (image_preds == labels).sum().item()\n",
    "\n",
    "            # 多模态融合\n",
    "            _, fusion_preds = torch.max(fused_logits, 1)\n",
    "            fusion_correct += (fusion_preds == labels).sum().item()\n",
    "\n",
    "            total += labels.size(0)\n",
    "\n",
    "    print(f\"\\n消融实验结果:\")\n",
    "    print(f\"仅文本模态准确率: {100 * text_correct / total:.2f}%\")\n",
    "    print(f\"仅图像模态准确率: {100 * image_correct / total:.2f}%\")\n",
    "    print(f\"多模态融合准确率: {100 * fusion_correct / total:.2f}%\")\n",
    "    print(f\"多模态相对提升: {100 * fusion_correct / total - max(100 * text_correct / total, 100 * image_correct / total):.2f}%\")\n",
    "\n",
    "    # ========== 9. 测试集预测 ==========\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"测试集预测\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    class FixedTestDataset(Dataset):\n",
    "        \"\"\"修复的测试数据集类\"\"\"\n",
    "        def __init__(self, test_file, data_dir, text_processor, image_processor):\n",
    "            self.data_dir = data_dir\n",
    "            self.text_processor = text_processor\n",
    "            self.image_processor = image_processor\n",
    "\n",
    "            # 读取测试文件\n",
    "            self.guids = []\n",
    "            try:\n",
    "                with open(test_file, 'r', encoding='utf-8') as f:\n",
    "                    lines = f.readlines()[1:]  # 跳过标题行\n",
    "                    for line in lines:\n",
    "                        guid = line.strip().split(',')[0]\n",
    "                        self.guids.append(guid)\n",
    "                print(f\"  测试集GUID数量: {len(self.guids)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  读取测试文件失败: {e}\")\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.guids)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            guid = self.guids[idx]\n",
    "\n",
    "            # 文本路径和图像路径\n",
    "            text_path = os.path.join(self.data_dir, f'{guid}.txt')\n",
    "            image_path = os.path.join(self.data_dir, f'{guid}.jpg')\n",
    "\n",
    "            # 处理文本\n",
    "            text_data = self.text_processor.preprocess(text_path)\n",
    "\n",
    "            # 处理图像\n",
    "            try:\n",
    "                image = self.image_processor.preprocess(image_path, augment=False)\n",
    "            except:\n",
    "                image = torch.zeros(3, 224, 224)\n",
    "\n",
    "            return {\n",
    "                'input_ids': text_data['input_ids'],\n",
    "                'attention_mask': text_data['attention_mask'],\n",
    "                'image': image,\n",
    "                'guid': guid\n",
    "            }\n",
    "\n",
    "    def fixed_predict_test_set(model, test_file, output_file):\n",
    "        \"\"\"修复的测试集预测函数\"\"\"\n",
    "        model.eval()\n",
    "\n",
    "        # 创建测试数据集\n",
    "        test_dataset = FixedTestDataset(\n",
    "            test_file=test_file,\n",
    "            data_dir='./data/data',\n",
    "            text_processor=text_processor,\n",
    "            image_processor=image_processor\n",
    "        )\n",
    "\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset, batch_size=16, shuffle=False, num_workers=0\n",
    "        )\n",
    "\n",
    "        # 预测\n",
    "        predictions = {}\n",
    "        label_mapping = {0: 'positive', 1: 'neutral', 2: 'negative'}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(test_loader, desc='预测'):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                images = batch['image'].to(device)\n",
    "                guids = batch['guid']\n",
    "\n",
    "                fused_logits, _, _ = model(input_ids, attention_mask, images)\n",
    "                _, preds = torch.max(fused_logits, 1)\n",
    "\n",
    "                for guid, pred in zip(guids, preds.cpu().numpy()):\n",
    "                    predictions[guid] = label_mapping.get(pred, 'neutral')\n",
    "\n",
    "        # 写入结果文件\n",
    "        try:\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(\"guid,tag\\n\")\n",
    "                with open(test_file, 'r', encoding='utf-8') as test_f:\n",
    "                    lines = test_f.readlines()[1:]\n",
    "                    for line in lines:\n",
    "                        guid = line.strip().split(',')[0]\n",
    "                        tag = predictions.get(guid, 'neutral')\n",
    "                        f.write(f\"{guid},{tag}\\n\")\n",
    "\n",
    "            print(f\"✅ 预测结果已保存到: {output_file}\")\n",
    "            print(f\"预测样本数量: {len(predictions)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"写入结果文件失败: {e}\")\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    # 执行预测\n",
    "    if os.path.exists('./data/test_without_label.txt'):\n",
    "        print(\"\\n开始预测测试集...\")\n",
    "        predictions = fixed_predict_test_set(\n",
    "            model,\n",
    "            test_file='./data/test_without_label.txt',\n",
    "            output_file='./fixed_submission.txt'\n",
    "        )\n",
    "    else:\n",
    "        print(\"\\n⚠️ 测试文件不存在，跳过预测步骤\")\n",
    "\n",
    "    # ========== 10. 实验总结 ==========\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"实验总结\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(f\"\\n实验配置:\")\n",
    "    print(f\"  - 训练样本: {len(train_dataset)}\")\n",
    "    print(f\"  - 验证样本: {len(val_dataset)}\")\n",
    "    print(f\"  - Batch Size: 16\")\n",
    "    print(f\"  - 学习率: 2e-5\")\n",
    "    print(f\"  - 融合方法: late\")\n",
    "    print(f\"  - 训练轮数: {epoch + 1}\")\n",
    "\n",
    "    print(f\"\\n最佳结果:\")\n",
    "    print(f\"  - 最佳验证准确率: {best_acc:.2f}%\")\n",
    "\n",
    "    print(f\"\\n生成文件:\")\n",
    "    print(f\"  - 最佳模型: fixed_best_model.pth\")\n",
    "    print(f\"  - 预测结果: fixed_submission.txt\")\n",
    "\n",
    "    print(\"\\n✅ 实验完成！\")\n",
    "    print(\"=\" * 60)"
   ],
   "id": "dc97c3f29e0eebbd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "多模态情感分类实验 - 完整可运行版本\n",
      "============================================================\n",
      "\n",
      "1. 环境设置...\n",
      "\n",
      "2. 初始化BERT模型...\n",
      "\n",
      "3. 初始化图像处理器...\n",
      "\n",
      "4. 创建数据集...\n",
      "\n",
      "5. 初始化模型...\n",
      "\n",
      "6. 设置训练参数...\n",
      "\n",
      "7. 开始主训练流程...\n",
      "  尝试从本地加载BERT tokenizer...\n",
      "  ✅ 本地BERT tokenizer加载成功\n",
      "\n",
      "加载训练数据...\n",
      "  ✅ 加载了 1000 个样本\n",
      "训练集大小: 800\n",
      "验证集大小: 200\n",
      "  初始化文本编码器...\n",
      "  初始化图像编码器...\n",
      "  ✅ 模型初始化完成，融合方法: late\n",
      "\n",
      "============================================================\n",
      "开始训练循环\n",
      "============================================================\n",
      "\n",
      "Epoch 1/10\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练: 100%|██████████| 50/50 [00:28<00:00,  1.75it/s, loss=1.3449, acc=52.1%]\n",
      "验证: 100%|██████████| 13/13 [00:02<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "分类报告:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive     0.6224    0.9919    0.7649       123\n",
      "     neutral     0.0000    0.0000    0.0000        21\n",
      "    negative     0.5000    0.0357    0.0667        56\n",
      "\n",
      "    accuracy                         0.6200       200\n",
      "   macro avg     0.3741    0.3425    0.2772       200\n",
      "weighted avg     0.5228    0.6200    0.4891       200\n",
      "\n",
      "\n",
      "训练集 - Loss: 1.4091, Acc: 52.12%\n",
      "验证集 - Loss: 0.8650, Acc: 62.00%\n",
      "✅ 保存最佳模型，验证准确率: 62.00%\n",
      "\n",
      "Epoch 2/10\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练: 100%|██████████| 50/50 [00:28<00:00,  1.76it/s, loss=1.4789, acc=66.6%]\n",
      "验证: 100%|██████████| 13/13 [00:02<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "分类报告:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive     0.6964    0.9512    0.8041       123\n",
      "     neutral     0.0000    0.0000    0.0000        21\n",
      "    negative     0.5312    0.3036    0.3864        56\n",
      "\n",
      "    accuracy                         0.6700       200\n",
      "   macro avg     0.4092    0.4183    0.3968       200\n",
      "weighted avg     0.5771    0.6700    0.6027       200\n",
      "\n",
      "\n",
      "训练集 - Loss: 1.1675, Acc: 66.62%\n",
      "验证集 - Loss: 0.7902, Acc: 67.00%\n",
      "✅ 保存最佳模型，验证准确率: 67.00%\n",
      "\n",
      "Epoch 3/10\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练: 100%|██████████| 50/50 [00:27<00:00,  1.80it/s, loss=0.9718, acc=77.6%]\n",
      "验证: 100%|██████████| 13/13 [00:02<00:00,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "分类报告:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive     0.7591    0.8455    0.8000       123\n",
      "     neutral     0.0000    0.0000    0.0000        21\n",
      "    negative     0.5238    0.5893    0.5546        56\n",
      "\n",
      "    accuracy                         0.6850       200\n",
      "   macro avg     0.4276    0.4783    0.4515       200\n",
      "weighted avg     0.6135    0.6850    0.6473       200\n",
      "\n",
      "\n",
      "训练集 - Loss: 0.9458, Acc: 77.62%\n",
      "验证集 - Loss: 0.7484, Acc: 68.50%\n",
      "✅ 保存最佳模型，验证准确率: 68.50%\n",
      "\n",
      "Epoch 4/10\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练: 100%|██████████| 50/50 [00:29<00:00,  1.69it/s, loss=0.7972, acc=86.2%]\n",
      "验证: 100%|██████████| 13/13 [00:02<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "分类报告:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive     0.7664    0.8537    0.8077       123\n",
      "     neutral     0.6667    0.0952    0.1667        21\n",
      "    negative     0.5333    0.5714    0.5517        56\n",
      "\n",
      "    accuracy                         0.6950       200\n",
      "   macro avg     0.6555    0.5068    0.5087       200\n",
      "weighted avg     0.6907    0.6950    0.6687       200\n",
      "\n",
      "\n",
      "训练集 - Loss: 0.7502, Acc: 86.25%\n",
      "验证集 - Loss: 0.7529, Acc: 69.50%\n",
      "✅ 保存最佳模型，验证准确率: 69.50%\n",
      "\n",
      "Epoch 5/10\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练: 100%|██████████| 50/50 [00:27<00:00,  1.81it/s, loss=0.8536, acc=90.2%]\n",
      "验证: 100%|██████████| 13/13 [00:02<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "分类报告:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive     0.7669    0.8293    0.7969       123\n",
      "     neutral     0.8000    0.1905    0.3077        21\n",
      "    negative     0.5000    0.5536    0.5254        56\n",
      "\n",
      "    accuracy                         0.6850       200\n",
      "   macro avg     0.6890    0.5244    0.5433       200\n",
      "weighted avg     0.6957    0.6850    0.6695       200\n",
      "\n",
      "\n",
      "训练集 - Loss: 0.6298, Acc: 90.25%\n",
      "验证集 - Loss: 0.7925, Acc: 68.50%\n",
      "\n",
      "Epoch 6/10\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练: 100%|██████████| 50/50 [00:27<00:00,  1.81it/s, loss=0.5501, acc=92.6%]\n",
      "验证: 100%|██████████| 13/13 [00:02<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "分类报告:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive     0.7500    0.8537    0.7985       123\n",
      "     neutral     0.6667    0.0952    0.1667        21\n",
      "    negative     0.5263    0.5357    0.5310        56\n",
      "\n",
      "    accuracy                         0.6850       200\n",
      "   macro avg     0.6477    0.4949    0.4987       200\n",
      "weighted avg     0.6786    0.6850    0.6572       200\n",
      "\n",
      "\n",
      "训练集 - Loss: 0.5549, Acc: 92.62%\n",
      "验证集 - Loss: 0.7902, Acc: 68.50%\n",
      "\n",
      "Epoch 7/10\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练: 100%|██████████| 50/50 [00:27<00:00,  1.81it/s, loss=0.6914, acc=93.5%]\n",
      "验证: 100%|██████████| 13/13 [00:02<00:00,  4.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "分类报告:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive     0.7630    0.8374    0.7984       123\n",
      "     neutral     0.8333    0.2381    0.3704        21\n",
      "    negative     0.5254    0.5536    0.5391        56\n",
      "\n",
      "    accuracy                         0.6950       200\n",
      "   macro avg     0.7072    0.5430    0.5693       200\n",
      "weighted avg     0.7038    0.6950    0.6809       200\n",
      "\n",
      "\n",
      "训练集 - Loss: 0.5031, Acc: 93.50%\n",
      "验证集 - Loss: 0.7999, Acc: 69.50%\n",
      "⚠️ 早停触发，连续3个epoch验证准确率未提升\n",
      "\n",
      "============================================================\n",
      "消融实验\n",
      "============================================================\n",
      "加载最佳模型进行消融实验...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "消融实验: 100%|██████████| 13/13 [00:02<00:00,  4.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "消融实验结果:\n",
      "仅文本模态准确率: 69.00%\n",
      "仅图像模态准确率: 61.00%\n",
      "多模态融合准确率: 69.50%\n",
      "多模态相对提升: 0.50%\n",
      "\n",
      "============================================================\n",
      "测试集预测\n",
      "============================================================\n",
      "\n",
      "开始预测测试集...\n",
      "  测试集GUID数量: 511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "预测: 100%|██████████| 32/32 [00:07<00:00,  4.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 预测结果已保存到: ./fixed_submission.txt\n",
      "预测样本数量: 511\n",
      "\n",
      "============================================================\n",
      "实验总结\n",
      "============================================================\n",
      "\n",
      "实验配置:\n",
      "  - 训练样本: 800\n",
      "  - 验证样本: 200\n",
      "  - Batch Size: 16\n",
      "  - 学习率: 2e-5\n",
      "  - 融合方法: late\n",
      "  - 训练轮数: 7\n",
      "\n",
      "最佳结果:\n",
      "  - 最佳验证准确率: 69.50%\n",
      "\n",
      "生成文件:\n",
      "  - 最佳模型: fixed_best_model.pth\n",
      "  - 预测结果: fixed_submission.txt\n",
      "\n",
      "✅ 实验完成！\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6ebf113c2920bbe2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
